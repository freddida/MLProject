{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook for a Neural Network Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0df0795dd2cde5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import necessary functions and modules\n",
    "from src.utils.check_mps_device import check_mps_device\n",
    "from src.utils.data_loading import load_data\n",
    "from src.utils.filtering import filter_data\n",
    "from src.utils.label_encoding import label_encode_column\n",
    "import numpy as np\n",
    "\n",
    "# Check if PyTorch Multi-Process Service (MPS) is available (GPU)\n",
    "check_mps_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d02b4b2bfed554f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a) Selected Features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc2be1747f70623f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define a list of selected features for analysis from the feature_selection notebook\n",
    "selected_features = [\n",
    "    \"geocentric_latitude\",  # Latitude of conjunction point [deg]\n",
    "    \"c_sigma_rdot\",  # Covariance: radial velocity standard deviation (sigma) of chaser [m/s]\n",
    "    \"c_obs_used\",  # Number of observations used for orbit determination (per CDM) of chaser\n",
    "    \"c_time_lastob_start\",\n",
    "    # Start of the time in days of the last accepted observation used in the orbit determination of chaser\n",
    "    \"c_time_lastob_end\",\n",
    "    # End of the time interval in days of the last accepted observation used in the orbit determination of chaser\n",
    "    \"mahalanobis_distance\",  # The distance between the chaser and target\n",
    "    \"miss_distance\",  # Relative position between chaser & target at TCA [m]\n",
    "    \"time_to_tca\",  # Time interval between CDM creation and time-of-closest approach [days]\n",
    "    \"t_cndot_r\",  # Covariance: correlation of normal (cross-track) velocity vs radial position of chaser\n",
    "    \"c_cr_area_over_mass\",  # Solar radiation coefficient . A/m (ballistic coefficient equivalent) of chaser\n",
    "    \"max_risk_estimate\",  # Maximum collision probability obtained by scaling combined covariance\n",
    "    \"c_span\",  # Size used by the collision risk computation algorithm of chaser [m]\n",
    "    \"max_risk_scaling\",  # Scaling factor used to compute maximum collision probability\n",
    "    \"t_rcs_estimate\",  # Radar cross-sectional area [m^2] of target\n",
    "    \"c_sigma_t\",  # Covariance: transverse (along-track) position standard deviation (sigma) of chaser [m]\n",
    "    \"c_obs_available\",  # Number of observations available for orbit determination (per CDM)\n",
    "    \"risk\",  # Risk value\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efaacfa08cb5d5f1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b) Data Loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47418c5418c1d6f0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the data and filter it\n",
    "df = load_data()  # Load the raw data into a DataFrame\n",
    "df_filtered = filter_data(\n",
    "    df\n",
    ")  # Filter the data according to criteria stated in the challenge\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_filtered.dropna(axis=0, how=\"any\", inplace=True)\n",
    "\n",
    "# Label encode the categorical column \"c_object_type\"\n",
    "label_encode_column(\n",
    "    df_filtered, \"c_object_type\"\n",
    ")  # Convert categorical column to numerical values\n",
    "\n",
    "# Extract the selected features and target variable\n",
    "df_processed = df_filtered[\n",
    "    selected_features\n",
    "]  # Select only the specified features for analysis\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df_processed.drop(\"risk\", axis=1)  # Features (17 columns)\n",
    "y = df_processed[\"risk\"]  # Target variable (risk)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd101080fa6f3345",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High/low risk distribution of original and preprocessed data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bc4642d9e1f46c9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from src.utils.plot_distrubution import plot_distribution\n",
    "\n",
    "plot_distribution(y, threshold=-6)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51b459f47195e41a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## c) Data Splitting into Training, Evaluation and Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5b68c4f46055cdb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert risk values into binary classes based on the threshold specified in the challenge\n",
    "y_class = np.where(y >= -6, 1, 0)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "# 'stratify' ensures proportional class distribution in train-test split using 'y_class'.\n",
    "X_train_eval, X_test, y_train_eval, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y_class\n",
    ")\n",
    "\n",
    "print(\"Training and Evaluation Set Shapes:\")\n",
    "print(X_train_eval.shape)  # Shape of the features in the training and evaluation set\n",
    "print(\n",
    "    y_train_eval.shape\n",
    ")  # Shape of the target variable in the training and evaluation set\n",
    "print(\"\\nTesting Set Shapes:\")\n",
    "print(X_test.shape)  # Shape of the features in the testing set\n",
    "print(y_test.shape)  # Shape of the target variable in the testing set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5148189349077432",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert risk values into binary classes based on the threshold specified in the challenge\n",
    "y_class_val = np.where(y_train_eval >= -6, 1, 0)\n",
    "\n",
    "# Split the remaining 80% into training and evaluation/validation sets (80/20 split) using stratify again\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_eval,\n",
    "    y_train_eval,\n",
    "    test_size=0.20,\n",
    "    random_state=21,\n",
    "    shuffle=True,\n",
    "    stratify=y_class_val,\n",
    ")\n",
    "\n",
    "print(\"Training Set Shapes:\")\n",
    "print(X_train.shape)  # Shape of the features in the training set\n",
    "print(y_train.shape)  # Shape of the target variable in the training set\n",
    "print(\"\\nValidation Set Shapes:\")\n",
    "print(X_val.shape)  # Shape of the features in the validation set\n",
    "print(y_val.shape)  # Shape of the target variable in the validation set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ca23e74ce819dca",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High/low risk distribution of training set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20baa9fba383b7bb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert risk values into binary classes for the training set\n",
    "y_train_class = np.where(y_train >= -6, 1, 0)\n",
    "\n",
    "# Print the shape of the binary classes array\n",
    "print(\"Shape of y_train_class:\", y_train_class.shape)\n",
    "\n",
    "# Display the distribution of the binary classes\n",
    "plot_distribution(y_train_class, threshold=0.5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35a2bb0db61bc48d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High/low risk distribution of validation set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79a3727d34e73f7b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert risk values into binary classes for the validation set\n",
    "y_eval_class = np.where(y_val >= -6, 1, 0)\n",
    "\n",
    "# Print the shape of the binary classes array\n",
    "print(\"Shape of y_eval_class:\", y_eval_class.shape)\n",
    "\n",
    "# Display the distribution of the binary classes\n",
    "plot_distribution(y_eval_class, threshold=0.5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "930d1003f35ca620",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High/low risk distribution of testing set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f523764f614851bc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert risk values into binary classes for the testing set\n",
    "y_test_class = np.where(y_test >= -6, 1, 0)\n",
    "\n",
    "# Print the shape of the binary classes array\n",
    "print(\"Shape of y_test_class:\", y_test_class.shape)\n",
    "\n",
    "# Display the distribution of the binary classes\n",
    "plot_distribution(y_test_class, threshold=0.5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a31895cbc1aa5ae",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## d) Over Sampling with SMOTE (Synthetic Minority Over-sampling Technique)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1187a18766ac3523"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[SMOTE](https://www.jair.org/index.php/jair/article/view/11192) is a technique used to address class imbalance in machine learning datasets, particularly in classification tasks. It works by generating synthetic samples for the minority class, thereby balancing the class distribution. SMOTE selects minority class instances and generates synthetic samples by interpolating between these instances and their nearest neighbors. This helps in mitigating the problem of class imbalance and improving the performance of machine learning models, especially those sensitive to class distribution."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b35e06f61c43b550"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize the SMOTE object with specified parameters\n",
    "smote = SMOTE(\n",
    "    random_state=42, k_neighbors=30\n",
    ")  # Number of nearest neighbors to use for generating synthetic samples\n",
    "\n",
    "# Resample the training data using SMOTE using unscaled training features and binary representation of trained targets\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train_class)\n",
    "\n",
    "# Print the shapes of the resampled training data\n",
    "print(\"Resampled Training Set Shapes:\")\n",
    "print(X_train_resampled.shape)  # Shape of the resampled features in the training set\n",
    "print(\n",
    "    y_train_resampled.shape\n",
    ")  # Shape of the resampled target variable in the training set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af3e8e5105f8a454",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High/low risk distribution of resampled training set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a9a97b8b148cdd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display the distribution of the resampled binary classes\n",
    "plot_distribution(y_train_resampled, threshold=0.5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee6868cd92d80a1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## e) Scaling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c330ed38fb2e4b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"Resampled Training Set Shape:\", y_train_resampled.shape)\n",
    "print(\"Evaluation Set Shape:\", y_train_eval.shape)\n",
    "print(\"Validation Set Shape:\", y_val.shape)\n",
    "print(\"Testing Set Shape:\", y_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75cb0220c39a909",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "# MinMaxScaler is used to scale the features to a range between 0 and 1 (uniformity across features)\n",
    "feature_scaler = MinMaxScaler()\n",
    "\n",
    "# Reshape X to ensure it's a 2D array for the scaler's fit_transform method\n",
    "X_train_rescaled = feature_scaler.fit_transform(X_train_resampled)\n",
    "X_train_eval_rescaled = feature_scaler.transform(X_train_eval)\n",
    "X_eval_rescaled = feature_scaler.transform(X_val)\n",
    "X_test_rescaled = feature_scaler.transform(X_test)\n",
    "\n",
    "print(\"Shape of X_train_rescaled:\", X_train_rescaled.shape)\n",
    "print(\"Shape of X_train_eval_rescaled:\", X_train_eval_rescaled.shape)\n",
    "print(\"Shape of X_eval_rescaled:\", X_eval_rescaled.shape)\n",
    "print(\"Shape of X_test_rescaled:\", X_test_rescaled.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0aef9202a035b94",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cc6da3b1416d044"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model is a feedforward neural network with an input layer, one hidden layer, and an output layer. The architecture includes:\n",
    "\n",
    "- **Input Layer**: 64 neurons with ReLU activation.\n",
    "- **Hidden Layer**: 32 neurons with ReLU activation.\n",
    "- **Output Layer**: 1 neuron with linear activation for regression.\n",
    "\n",
    "The model is compiled using the Adam optimizer (learning rate = 0.001) and mean squared error loss."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13ecf5f6d1390839"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a) Build Neural Network Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d578cc0825e42ccf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### v1.0.0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32448f7ca7c2e0b5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.optimizers.legacy import Adam\n",
    "#\n",
    "# # Define the neural network model: linear stack of layers\n",
    "# model = Sequential()\n",
    "#\n",
    "# # Add input layer and hidden layers\n",
    "# model.add(Dense(64, input_dim=X_train_rescaled.shape[1],\n",
    "#                 activation='relu'))  # Input layer with 64 neurons and ReLU activation\n",
    "# model.add(Dense(32, activation='relu'))  # Hidden layer with 32 neurons and ReLU activation\n",
    "#\n",
    "# # Add output layer\n",
    "# # sigmoid for binary classification\n",
    "# model.add(Dense(1, activation='sigmoid'))  # Output layer with linear activation for regression task\n",
    "#\n",
    "# # Compile the model\n",
    "# # binary_crossentropy: Used for binary classification problems, computes loss based on probability distributions of binary classes.\n",
    "# model.compile(optimizer=Adam(learning_rate=0.001),  # Adam optimizer with learning rate of 0.001\n",
    "#               loss='binary_crossentropy', metrics=['accuracy'])  # Mean squared error loss for regression\n",
    "#\n",
    "# # Print model summary\n",
    "# model.summary()  # Display the architecture and parameters of the neural network model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cb4c226d9ea9097",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### v2.0.0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d6873ce9ed41654"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer and hidden layers\n",
    "model.add(Dense(124, input_dim=X_train_rescaled.shape[1], activation=\"relu\"))\n",
    "model.add(BatchNormalization())  # Batch normalization layer\n",
    "model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add output layer\n",
    "# sigmoid for binary classification\n",
    "model.add(Dense(1, activation=\"sigmoid\"))  # Linear activation for regression task\n",
    "\n",
    "# Compile the model\n",
    "# binary_crossentropy: Used for binary classification problems, computes loss based on probability distributions of binary classes.\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")  # Mean squared error loss for regression\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3719779c376e0d8a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b) Train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d03641093328002"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**fit():**\n",
    "- During training, the model is repeatedly exposed to batches of training data (X_train_rescaled and y_train_resampled). For each batch, the model computes predictions, calculates the loss (difference between predicted and actual values), and updates the model's parameters (weights and biases) using an optimization algorithm (e.g., Adam optimizer) to minimize the loss.\n",
    "\n",
    "- The validation data (X_eval_rescaled and y_eval_class) are used periodically (after each epoch) to evaluate the model's performance on unseen data. This helps monitor the model's generalization ability and detect overfitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe5165e986dc8263"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# X_train_rescaled: Scaled features of the training set\n",
    "# y_train_resampled: Binary classes (after oversampling) corresponding to the training set\n",
    "# validation_data=(X_eval_rescaled, y_eval_class): Data used for validation during training\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_rescaled,\n",
    "    y_train_resampled,\n",
    "    shuffle=True,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_eval_rescaled, y_eval_class),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84eadf83b271445c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the accuracy of the model for the Training and Validation data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fbf2b847d6c21b3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy with selected features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"Training\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "# Set y-axis limits from 0 to 1\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Show figure\n",
    "# Set the facecolor to white\n",
    "plt.gcf().set_facecolor(\"white\")\n",
    "\n",
    "# Save the figure with a white background\n",
    "# plt.savefig(\"model_accuracy_feature.png\", facecolor='white')\n",
    "\n",
    "# Show figure\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed6346688ad6d634",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the loss of the model for the Training and Validation data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28f0d59a00a4a097"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Loss with selected features\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend([\"Training\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "# Show figure\n",
    "plt.gcf().set_facecolor(\"white\")\n",
    "\n",
    "# Save the figure with a white background\n",
    "# plt.savefig(\"loss_feature.png\", facecolor='white')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb6c4ef71f33f459",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## c) Predicting the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1408a513198c53ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Predict using the trained model on rescaled testing data\n",
    "risk_predictions = model.predict(X_test_rescaled)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "905b8ce15425ac4b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High/low risk distribution of predicted risk without classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97ced0701ed306a8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display the distribution of the predicted risk\n",
    "plot_distribution(risk_predictions, classification=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "375ebdaf9288d1b0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Find best threshold for binary classification for F-beta score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42b73501323ff082"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from src.utils.calculate_threshold import find_best_threshold\n",
    "\n",
    "best_threshold, best_score = find_best_threshold(y_test_class, risk_predictions)\n",
    "\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Best F-beta Score:\", best_score)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "942f75cb853cb6c2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High/low risk distribution of risk predictions with computed threshold"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c8eec3d277bdb34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display the distribution of the resampled binary classes\n",
    "plot_distribution(risk_predictions, threshold=best_threshold)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4577a5037c42f04a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) Evaluation of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b2ed0b7600efdca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**F-beta score**:\n",
    "\n",
    "It's a metric that balances precision and recall, with the emphasis on recall. By using the beta parameter (here, set to 2), we're placing more importance on recall than precision. This is crucial in scenarios where false negatives (missed positive cases) are more costly than false positives."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "457933cfef959001"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Explanation for the values [-5, -6.0001]:**\n",
    "\n",
    "Took from the [Case Study](https://github.com/jaimeperezsanchez/Collision_Avoidance_Challenge_Deep_Learning/blob/master/Technical_Report.pdf):\n",
    "For the MSE computation, we set all the low-risk prediction as close as possible to the threshold (risk greater or equal to -6). This makes the scores improve a lot as the maximum difference while computing the MSE between two values is now -6 (the high-risk range) instead of -30 (the total risk range)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73957dd01fec3104"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from src.utils.calculate_statistics import calculate_statistics_for_evaluation\n",
    "\n",
    "predictions_real = np.where(risk_predictions >= best_threshold, -5, -6.0001)\n",
    "\n",
    "# Calculate F-beta and MSE_HR scores\n",
    "f_beta = calculate_statistics_for_evaluation(y_pred=predictions_real, y_true=y_test)\n",
    "print(\"F-beta score: \", f_beta)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9fb7c5ed2eeed62",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
