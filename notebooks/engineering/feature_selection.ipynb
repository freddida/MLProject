{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook for feature selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63741f1ccb7d9e88"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import of all necessary libaries and check if MPS is available for faster computation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c2891a627a931e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d6c25deb83fa7eda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.utils.check_mps_device import check_mps_device\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils.data_loading import load_data\n",
    "from src.utils.filtering import filter_data\n",
    "\n",
    "from src.utils.label_encoding import label_encode_column\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if PyTorch Multi-Process Service (MPS) is available (GPU)\n",
    "check_mps_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following four different Feature Selection Techniques are applied.\n",
    "The first three (Univariate Selection, Feature Importance, Correlation) are explained in this [Article](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e)\n",
    "\n",
    "The fourth (quantum-classical hybrid solver) follows the [dwave-scikit-learn-plugin](https://github.com/dwavesystems/dwave-scikit-learn-plugin)#"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3c9c8fbc6555f18"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "59ec1a09cf9579a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Univariate Selection\n",
    "This methods makes use of the [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) class that evaluates the top k features based on the [mutual_info_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html) function.\n",
    "It measures the amount of information shared between two variables, and computes the mutual information between each feature and the target variable in a classification task [ref](https://medium.com/@Kavya2099/optimizing-performance-selectkbest-for-efficient-feature-selection-in-machine-learning-3b635905ed48#:~:text=SelectKBest%20uses%20statistical%20tests%20like,in%20the%20final%20feature%20subset).\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c05ff0751393fcf8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "# Load and data and filter it\n",
    "df = load_data()\n",
    "df_filtered = filter_data(df)\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_filtered.dropna(axis=0, how=\"any\", inplace=True)\n",
    "\n",
    "# Label encode the categorical column \"c_object_type\"\n",
    "label_encode_column(df_filtered, \"c_object_type\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df_filtered.drop([\"risk\"], axis=1)\n",
    "y = df_filtered[\"risk\"]\n",
    "\n",
    "# Feature selection using SelectKBest with mutual_info_regression and all features (k='all')\n",
    "selector = SelectKBest(score_func=mutual_info_regression, k=\"all\")\n",
    "# Evaluate the scoring function for each feature in the dataset\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_feature_names = X.columns[selector.get_support()]\n",
    "\n",
    "# Get the feature scores\n",
    "feature_scores = selector.scores_\n",
    "\n",
    "# Create a DataFrame with selected features and their scores\n",
    "selected_features_df = pd.DataFrame(\n",
    "    {\"Feature\": selected_feature_names, \"Score\": feature_scores[selector.get_support()]}\n",
    ")\n",
    "selected_features_df = selected_features_df.sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "# Plot the feature scores using Seaborn\n",
    "plt.figure(figsize=(10, 20))\n",
    "sns.barplot(x=\"Score\", y=\"Feature\", data=selected_features_df, hue=\"Feature\")\n",
    "\n",
    "# Add a line to visualize the elbow point\n",
    "elbow_point = 6\n",
    "plt.axvline(\n",
    "    x=selected_features_df[\"Score\"].iloc[elbow_point - 1],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Elbow Point\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Mutual Information Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance with SelectKBest\")\n",
    "plt.show()\n",
    "\n",
    "# Select the top x features based on the elbow method\n",
    "selected_features_kbest = selected_features_df[: elbow_point - 1]\n",
    "selected_features_kbest = selected_features_kbest[\"Feature\"].tolist()\n",
    "print(f\"Top {elbow_point - 1} selected features:\", selected_features_kbest)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6adc750a442c4b1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Feature Importance with ExtraTreesRegressor\n",
    "This feature usess the [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) class.\n",
    "It builds a collection of decision trees and makes predictions by averaging the outputs of these individual trees. The \"Extra\" in Extra Trees refers to the fact that, unlike traditional Random Forests, it selects split points for nodes randomly, without any optimization process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64a5c3ca1d004951"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load and data and filter it\n",
    "df = load_data()\n",
    "df_filtered = filter_data(df)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_filtered.dropna(axis=0, how=\"any\", inplace=True)\n",
    "\n",
    "# Encode categorical column \"c_object_type\"\n",
    "label_encode_column(df_filtered, \"c_object_type\")\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df_filtered.drop([\"risk\"], axis=1)\n",
    "y = df_filtered[\"risk\"]\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train the ExtraTreesRegressor model\n",
    "model = ExtraTreesRegressor()\n",
    "model.fit(features_scaled, y)\n",
    "\n",
    "# Plot the feature scores using Seaborn\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "# Create a DataFrame with feature names and their importances\n",
    "feat_importances = pd.DataFrame(\n",
    "    {\"Feature\": X.columns, \"Importance\": model.feature_importances_}\n",
    ")\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feat_importances = feat_importances.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot the feature importances using Seaborn\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_importances, hue=\"Feature\")\n",
    "\n",
    "# Add a line to visualize the elbow point\n",
    "elbow_point = 8\n",
    "plt.axvline(\n",
    "    x=feat_importances[\"Importance\"].iloc[elbow_point - 1],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Elbow Point\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importances with ExtraTreesRegressor\")\n",
    "plt.show()\n",
    "\n",
    "# Select the top x features based on the elbow method\n",
    "selected_features_regressor = feat_importances[: elbow_point - 1]\n",
    "selected_features_regressor = selected_features_regressor[\"Feature\"].tolist()\n",
    "print(f\"Top {elbow_point - 1} selected features:\", selected_features_regressor)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb166eaee44b8e8c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Correlation Matrix\n",
    "The ```corr()``` method in pandas calculates the Pearson correlation coefficient by default. \n",
    "This coefficient quantifies the strength and direction of a linear relationship between two variables, ranging from -1 to 1:\n",
    "\n",
    "- 1: Perfect positive correlation (both variables increase linearly).\n",
    "- 0: No linear correlation.\n",
    "- -1: Perfect negative correlation (one variable increases, the other decreases linearly)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5773774cd86f39d6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load and data and filter it\n",
    "df = load_data()\n",
    "df_filtered = filter_data(df)\n",
    "\n",
    "# Label encode a specific categorical column (\"c_object_type\")\n",
    "label_encode_column(df_filtered, \"c_object_type\")\n",
    "\n",
    "# Calculate the correlation matrix for the filtered data\n",
    "correlation_matrix = df_filtered.corr()\n",
    "\n",
    "# Identify features with high correlation with the target variable (\"risk\")\n",
    "target_correlation = correlation_matrix[\"risk\"].abs().sort_values(ascending=False)\n",
    "\n",
    "# Exclude the target variable to get the best features\n",
    "best_features = target_correlation[1:]\n",
    "\n",
    "# Plot the correlation scores of the top features\n",
    "plt.figure(figsize=(10, 20))\n",
    "sns.barplot(x=best_features.values, y=best_features.index, hue=best_features.index)\n",
    "\n",
    "elbow_point = 6\n",
    "plt.axvline(\n",
    "    x=best_features.iloc[elbow_point - 1],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Elbow Point\",\n",
    ")\n",
    "\n",
    "plt.title(\"Top Feature Correlation Scores\")\n",
    "plt.xlabel(\"Correlation Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# Select the top x features based on the elbow method\n",
    "selected_features_corr = best_features.index[: elbow_point - 1].tolist()\n",
    "print(f\"Top {elbow_point - 1} selected features:\", selected_features_corr)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8765c52afe259ab",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. D-Wave scikit-learn Plugin using a quantum-classical hybrid solver.\n",
    "This plugin selects features using a [quadratic optimization](https://docs.ocean.dwavesys.com/en/stable/concepts/cqm.html#constrained-quadratic-models) problem that is solved on a [hybrid solver](https://docs.ocean.dwavesys.com/en/stable/concepts/hybrid.html).\n",
    "\n",
    "To use the solver you need to:\n",
    "- Create a free account on [dwave](https://cloud.dwavesys.com/leap/signup/)\n",
    "- Create a [Configuration File](https://docs.ocean.dwavesys.com/en/stable/overview/sapi.html) with ```dwave config create --auto-token``` by adding your [Solver API Token](https://cloud.dwavesys.com/leap/).\n",
    "- Verify the configuration with ```dwave ping --client qpu```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "965638ca4de9c6f3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dwave.plugins.sklearn import SelectFromQuadraticModel\n",
    "from dwave.cloud import Client\n",
    "\n",
    "# Connect to D-Wave cloud service\n",
    "with Client.from_config() as client:\n",
    "    solver = client.get_solver()\n",
    "    print(\"Solver ID:\", solver.id)\n",
    "\n",
    "# Load and data and filter it\n",
    "df = load_data()\n",
    "df_filtered = filter_data(df)\n",
    "\n",
    "# Drop NaN values\n",
    "df_filtered.dropna(axis=0, how=\"any\", inplace=True)\n",
    "\n",
    "# Encode categorical column\n",
    "label_encode_column(df_filtered, \"c_object_type\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_filtered.drop([\"risk\", \"event_id\"], axis=1)  # Features\n",
    "y = df_filtered[\"risk\"]  # Target\n",
    "\n",
    "# Apply SelectFromQuadraticModel\n",
    "selector = SelectFromQuadraticModel(\n",
    "    num_features=10\n",
    ")  # Create a feature selection object\n",
    "X_new = selector.fit_transform(\n",
    "    X.values, y.values\n",
    ")  # Fit the selector to the data and transform it\n",
    "\n",
    "# Get the selected features\n",
    "selected_features_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Print the selected features\n",
    "selected_features = X.columns[selected_features_indices]\n",
    "\n",
    "selected_features_dwave = (\n",
    "    selected_features.tolist()\n",
    ")  # Convert selected features to a list\n",
    "\n",
    "print(f\"Top {len(selected_features)} selected features:\", selected_features)\n",
    "\n",
    "# ['time_to_tca', 'max_risk_estimate', 'miss_distance', 't_cndot_r',\n",
    "#        'c_time_lastob_start', 'c_time_lastob_end', 'c_obs_used',\n",
    "#        'c_cr_area_over_mass', 'geocentric_latitude', 'mahalanobis_distance']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cc2a272393b58d5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "afa77ef30cac2bec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selected features for further processing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7003904a8ce1557"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Combine all selected features into a single list without duplicates\n",
    "# Combine features from different methods into a list\n",
    "original_features_selection = list(\n",
    "    selected_features_corr\n",
    "    + selected_features_kbest\n",
    "    + selected_features_regressor\n",
    "    + selected_features_dwave\n",
    ")\n",
    "\n",
    "# Print the length of the list before removing duplicates\n",
    "print(\n",
    "    \"Number of features before removing duplicates:\", len(original_features_selection)\n",
    ")\n",
    "\n",
    "# Remove duplicates by converting the list to a set and then back to a list\n",
    "selected_features_final = list(\n",
    "    set(\n",
    "        selected_features_corr\n",
    "        + selected_features_kbest\n",
    "        + selected_features_regressor\n",
    "        + selected_features_dwave\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the length of the final list after removing duplicates\n",
    "print(\"Number of features after removing duplicates:\", len(selected_features_final))\n",
    "\n",
    "# Print or use the new list\n",
    "print(\"All Selected Features:\")\n",
    "print(selected_features_final)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f744580d239c398d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cc2055cf2c5cba98"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8732224d75ce2189"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": ".venv",
   "language": "python",
   "display_name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
